\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{part}{1}

\begin{document}




\part{Serre}
\chapter{Generalities on Linear Representations}
\section{Definitions}
\begin{itemize}
    \item \marginnote{7/15:}$\bm{GL(V)}$: The group of isomorphisms of $V$ onto itself, where $V$ is a vector space over the field $\C$ of complex numbers.
    \item If $(e_i)$ is a finite basis of $n$ elements for $V$, then each linear map $a:V\to V$ is defined by a square matrix $(a_{ij})$ of order $n$.
    \item The coefficients $a_{ij}$ are complex number derived from expressing the images $a(e_j)$ in terms of the basis $(e_i)$ and solving, i.e., we know that each $a(e_j)=\sum_ia_{ij}e_i$.
    \item $a$ is an isomorphism $\Longleftrightarrow$ $\det(a)=\det(a_{ij})\neq 0$.
    \item We can thus identify $GL(V)$ with the group of invertible square matrices of order $n$.
    \item \textbf{Linear representation} (of $G$ in $V$): A homomorphism $\rho:G\to GL(V)$, where $G$ is a finite group.
    \item \textbf{Representation space} (of $G$): The vector space $V$, given a homomorphism $\rho$. \emph{Also known as} \textbf{representation}.
    \item \textbf{Degree} (of a representation $V$): The dimension of the representation space.
    \item \textbf{Similar} (representations): Two representations $\rho,\rho'$ of the same group $G$ in vector spaces $V$ and $V'$ such that there exists a linear isomorphism $\tau:V\to V'$ which satisfies the identity $\tau\circ\rho(s)=\rho'(s)\circ\tau$ for all $s\in G$. \emph{Also known as} \textbf{isomorphic}.
    \begin{itemize}
        \item When $\rho(s),\rho'(s)$ are given in matrix form by $R_s,R_s'$, respectively, this means that there exists an invertible matrix $T$ such that $T\cdot R_s=R_s'\cdot T$ for all $s\in G$.
    \end{itemize}
\end{itemize}


\section{Basic Examples}
\begin{itemize}
    \item \textbf{Unit representation}: The representation $\rho$ of $G$ defined by $\rho(s)=1$ for all $s\in G$. \emph{Also known as} \textbf{trivial representation}.
    \item \textbf{Regular representation}: The representation $\rho$ defined by $\rho(s)=f:V\to V$ where $f:e_t\mapsto e_{st}$, where $V$ is a vector space of dimension $g=|G|$ with basis $(e_t)_{t\in G}$.
    \item \textbf{Permutation representation}: The representation $\rho$ defined by $\rho(s)=f:V\to V$, where $f:e_x\to e_{sx}$, $x\in X$ being the set acted upon by $G$.
\end{itemize}


\begin{enumerate}
    \item So $\rho$ is the representation of a group, technically. But it seems like we more often treat $V$ as the representation. So which is it, because it seems like they are distinct concepts?
    \item What is the utility of representation theory in mathematics? Does mapping group elements onto automorphisms that obey similar properties in the more well-defined vector space allow us to prove certain results about groups, for instance? Is it the other way around, in that representation theory allows us to prove results about vector spaces from what we know about groups? Is representation theory purely a way of linking group theory and linear algebra so that results in one field may be applied to the other and vice versa? I'm just trying to wrap my head around the motivation for creating and studying homomorphism from groups to vector space automorphisms/linear transformations.
    \item Subrepresentations and blocks of block-diagonal matrices?
    \item Can you explain kernel to me?
    \item What about stability under subgroups of G?
    \item Equivalence between representation and group actions?
    \item Are linear automorphisms a generalization of permutations?
    \item It seems like the general linear group is a group, and all that representation theory does is maps an arbitrary group onto a general linear group in a manner that preserves the group operation. But why bother? If we wanted to study the group-like characteristics of the general linear group, couldn't we just do that directly? Or is the point to have a common reference point for a whole bunch of groups?
    \item We define a permutation to be a function because the "original set" and the "final set" are both critical to understanding the nature of what a permutation is. We do the same for a representation for the same reason?
\end{enumerate}

\begin{enumerate}
    \item Determinant of an element?
    \item Significance of similarity, conjugates in linear algebra? Are conjugates like change of basis?
    \item So although it doesn't have to be, a regular representation could be expressed such that every linear isomorphism is a permutation matrix? And if it isn't, we can do a change of basis to make it so?
    \item Relationship between the permutation representation and the regular representation. Are they basically equivalent?
    \item Proof of Section 1.3, Theorem 1?
    \item Definition of direct sum of two representations?
    \item What is the tensor product/Kronecker product?
    \item Every $\rho_s$ has finite order. Thus, the matrices $R_s$ don't really "grow," i.e., diverge to infinity. This implies eigenvalues equal to 1 and draws a comparison to the roots of unity representation (representation of degree 1). The matrices of a representation don't grow. They're all kind of like the roots of unity. What more tangible attributes are there of these matrices that constrain them to this very small subset of all possible linear transformations?
    \item The complex conjugate of a root of unity is equal to the inverse of said root.
    \item A character is a full function, as defined in Serre. In Chemistry, I was told that a character was an individual trace of an individual matrix. Can you clarify the distinction?
    \item It seems like one of the major thing holding me back is a solid understanding of conjugacy. Whatever you can do to expound upon this would be greatly appreciated.
    \item The majority of Chapter 2 was full of characters, symbols, terms, and logic that was completely foreign to me.
    \item I understand the surface level of most of what is being said about characters, but I can tell that there are major gaps in my background. For example, I have never done rigorous linear algebraic proofs, so I'm not super familiar with notions and implications of conjugates. When he says, "the well known fromula $Tr(ab)=Tr(ba)$," I have never heard of this before. So what kinds of things can I look into in a reasonable amount of time to deepen my understanding, or is it not really necessary at this point to fully understand what I'm reading? Note that I am going to start working through Axler's Linear Algebra Done Right in prep for Honors Analysis very soon, if you're familiar, so my knowledge may well begin deepening already.
\end{enumerate}




\end{document}